# -*- coding: utf-8 -*-
"""ML_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KIscABuLJunstgV1moNRMqrXxT-0y453
"""

from google.colab import drive
import glob
import json
import re
from transformers import AutoModel, AutoTokenizer, get_scheduler
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import torch
drive.mount('/content/drive')

files = glob.glob('/content/drive/MyDrive/aihub/TL1_aihub/talksets-train-*/*.json')

from transformers import AutoTokenizer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", trust_remote_code=True)
processed = []
label_list = ['CENSURE', 'HATE', 'DISCRIMINATION', 'SEXUAL', 'ABUSE', 'VIOLENCE', 'CRIME']
label2idx = {label: idx for idx, label in enumerate(label_list)}

for filename in files:
    with open(filename, "r", encoding="utf-8") as f:
        data = json.load(f)

    for conv in data:
        for sentence in conv.get("sentences", []):
            text = sentence.get("text")
            types = sentence.get("types", [])

            if text is None:
                print(f"âš ï¸ ëˆ„ë½ëœ ë°ì´í„°: {sentence}")
                continue

            # âœ… í† í¬ë‚˜ì´ì¦ˆ ê²°ê³¼ë¥¼ ì‚¬ì „ì— ì¶”ê°€
            encoding = tokenizer(
                text,
                padding="max_length",
                truncation=True,
                max_length=128
            )

            processed.append({
                "text": text,
                "types": types,
                "input_ids": encoding["input_ids"],
                "attention_mask": encoding["attention_mask"]
            })

import torch
from torch.utils.data import Dataset

class BaseDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __getitem__(self, idx):
        item = self.data[idx]

        # label ë²¡í„° ë§Œë“¤ê¸°
        label_vector = [0] * len(label2idx)
        for label in item["types"]:
            if label in label2idx:
                label_vector[label2idx[label]] = 1
        # ëª¨ë¸ì— ë„˜ê¸°ê¸° ìœ„í•´ tensor í˜•íƒœë¡œ ë³€í™˜, FloatTensorë¥¼ ì“°ëŠ” ì´ìœ ëŠ” nn.BCEWithLogitsLoss() ê°™ì€ ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ìš© loss í•¨ìˆ˜ê°€ float ê°’ì„ ìš”êµ¬í•˜ê¸° ë•Œë¬¸.
        return {
            "input_ids": torch.tensor(item["input_ids"]),
            "attention_mask": torch.tensor(item["attention_mask"]),
            "labels": torch.FloatTensor(label_vector)
        }

    def __len__(self):
        return len(self.data)

from collections import Counter
import random

random.seed(42)
target_count = 500

label_counts = Counter()
final_data = []
used_texts = set()

for item in processed:
    item_text = item["text"]
    if item_text in used_texts:
        continue

    add_sample = False
    temp_counts = label_counts.copy()
    for label in item["types"]:
        if label in label_list and label_counts[label] < target_count:
            add_sample = True
            break

    if not add_sample:
        continue

    # ì´ ì‹œì ì—ì„œ ìƒ˜í”Œì„ ì¶”ê°€
    final_data.append(item)
    used_texts.add(item_text)

    for label in item["types"]:
        if label in label_list and label_counts[label] < target_count:
            label_counts[label] += 1

    if all(label_counts[l] >= target_count for l in label_list):
        break

print("âœ… ë¼ë²¨ë³„ ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼:", label_counts)
print(f"ğŸ“Š ìµœì¢… ìƒ˜í”Œ ìˆ˜: {len(final_data)}")

# test ì „ìš© ìƒ˜í”Œ ì¶”ì¶œ
test_label_counts = Counter()
test_data = []
used_texts = set()
target_test_count = 100  # labelë‹¹ testìš©ìœ¼ë¡œ 100ê°œì”©

random.shuffle(final_data)  # final_dataì—ì„œ ì¶”ì¶œ

for item in final_data:
    item_text = item["text"]
    if item_text in used_texts:
        continue

    add = False
    for label in item["types"]:
        if label in label_list and test_label_counts[label] < target_test_count:
            add = True
            break

    if not add:
        continue

    test_data.append(item)
    used_texts.add(item_text)

    for label in item["types"]:
        if label in label_list and test_label_counts[label] < target_test_count:
            test_label_counts[label] += 1

    if all(test_label_counts[l] >= target_test_count for l in label_list):
        break

print(test_label_counts)  # âœ… ëª¨ë“  ë¼ë²¨ì´ 100ê°œì”© ë‚˜ì™€ì•¼ í•¨

from sklearn.model_selection import train_test_split

train_data, temp_data = train_test_split(final_data, test_size=0.2, random_state=42)
val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

# âœ… ê·¸ë¦¬ê³  ë°”ë¡œ BaseDataset êµ¬ì„±
train_dataset = BaseDataset(train_data)
val_dataset = BaseDataset(val_data)
test_dataset = BaseDataset(test_data)

import torch.nn as nn

#íŒŒì¸ íŠœë‹ì„ í•˜ëŠ” ëª¨ë¸, íŒŒì¸íŠœë‹ì€ ì´í›„ ì™¸ë¶€ì—ì„œ ì§„í–‰ë¨.
class KoBERTClassifier(nn.Module):
    def __init__(self, bert_model, num_labels):
        super(KoBERTClassifier, self).__init__()
        self.bert = bert_model #KoBERT ëª¨ë¸ì„ ì €ì¥
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels) # KoBERTë¥¼ CLSë²¡í„°ë¥¼ ë°›ì•„ num_labelsê°œì˜ ë¡œì§“ ì¶œë ¥

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.pooler_output  # CLS í† í° ë²¡í„°ëŠ” ë¬¸ì¥ì„ ëŒ€í‘œí•˜ëŠ” ë²¡í„°ë¥¼ ì˜ë¯¸í•¨.
        return self.classifier(cls_output)

!pip install pytorch-lightning
import pytorch_lightning as pl

def freeze_bert_layers(model, freeze_until=8):
    """
    KoBERTì˜ í•˜ìœ„ encoder.layer.0 ~ encoder.layer.(freeze_until - 1)ê¹Œì§€ freezing
    í•˜ìœ„ ë ˆì´ì–´ëŠ” ê³ ì •ì‹œí‚¤ê³  ìƒìœ„ ë ˆì´ì–´ì—ì„œë§Œ í•™ìŠµì‹œí‚´.
    """
    for name, param in model.bert.named_parameters():
        if name.startswith("encoder.layer."):
            try:
                layer_num = int(name.split(".")[2])
                if layer_num < freeze_until:
                    param.requires_grad = False
            except ValueError:
                continue

class FocalLoss(nn.Module):
    def __init__(self, gamma=1.0):
        super().__init__()
        self.gamma = gamma

    def forward(self, inputs, targets, alpha=None):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)
        focal_loss = ((1 - pt) ** self.gamma) * BCE_loss
        if alpha is not None:
            focal_loss = alpha * focal_loss
        return focal_loss.mean()


# í•™ìŠµ ë£¨í”„ì— ëŒ€í•œ í´ë˜ìŠ¤ë¡œ ëª¨ë¸ê³¼ í•™ìŠµ ë¡œì§ì„ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¡œ ë˜í•‘í•´ì¤Œ.
class HateSpeechLightningModel(pl.LightningModule):
    def __init__(self, model, lr=2e-5):
        super().__init__()
        self.model = model
        self.lr = lr

        # í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •
        support = torch.tensor([16389, 5419, 3280, 1856, 1592, 1502, 613], dtype=torch.float32)
        alpha = (1.0 / support)
        alpha = alpha / alpha.sum() * len(support)

        # alphaë¥¼ register_bufferë¡œ ë“±ë¡ â†’ GPUë¡œ ìë™ ì´ë™ë¨
        self.register_buffer("alpha", alpha)

        self.focal_loss = FocalLoss(gamma=1.0)

    def forward(self, input_ids, attention_mask):
        return self.model(input_ids=input_ids, attention_mask=attention_mask)

    def training_step(self, batch, batch_idx):
        logits = self(batch['input_ids'], batch['attention_mask'])
        loss = self.focal_loss(logits, batch['labels'], self.alpha)
        self.log("train_loss", loss, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        logits = self(batch['input_ids'], batch['attention_mask'])
        loss = self.focal_loss(logits, batch['labels'], self.alpha)
        self.log("val_loss", loss, prog_bar=True)
        return loss

    def configure_optimizers(self):
        return torch.optim.AdamW(self.parameters(), lr=self.lr)

import torch
import torch.nn.functional as F
from sklearn.metrics import f1_score, precision_score, recall_score
from pytorch_lightning import Trainer
from transformers import AutoModel

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)

# 1. ê¸°ì¡´ì˜ KoBERT base ëª¨ë¸ ë¡œë“œ(íŒŒì¸ íŠœë‹)
kobert_base = AutoModel.from_pretrained("monologg/kobert", trust_remote_code=True)

# 2. ë¶„ë¥˜ê¸° ëª¨ë¸ì— base ëª¨ë¸ í¬í•¨ì‹œí‚´
model = KoBERTClassifier(kobert_base, num_labels=len(label_list)).to(device)

# 3. freeze ì ìš©ì€ model.bertì— í•´ì•¼ í•¨.
freeze_bert_layers(model, freeze_until=8)

# 4. Lightning ë˜í•‘
lightning_model = HateSpeechLightningModel(model)

# 5. Trainer ì„¤ì • (ì†ë„ ìµœì í™” ë°˜ì˜ë¨)
trainer = Trainer(
    max_epochs=5,
    accelerator="auto",
    devices=1 if torch.cuda.is_available() else None,
    precision=16,  # mixed precision â†’ ì†ë„ í–¥ìƒ
    enable_checkpointing=False,  # ì €ì¥ ìƒëµ â†’ ë” ë¹ ë¦„
    logger=False,                # ë¡œê·¸ ìƒëµ â†’ ë” ë¹ ë¦„
    log_every_n_steps=10
)

# 6. í•™ìŠµ ì‹œì‘
trainer.fit(lightning_model, train_loader, val_loader)

test_dataset = BaseDataset(test_data)

test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)

from sklearn.metrics import classification_report


def evaluate_model(model, dataloader, device, threshold=0.5):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].cpu().numpy()

            logits = model(input_ids, attention_mask).cpu()
            probs = torch.sigmoid(logits).numpy()

            # thresholdê°€ scalarë©´ ì „ì²´ì— ë™ì¼ ì ìš© / arrayë©´ labelë§ˆë‹¤ ë‹¤ë¥´ê²Œ ì ìš©
            if isinstance(threshold, (list, np.ndarray)):
                preds = (probs > np.array(threshold)).astype(int)
            else:
                preds = (probs > threshold).astype(int)

            all_preds.append(preds)
            all_labels.append(labels)

    y_true = np.vstack(all_labels)
    y_pred = np.vstack(all_preds)

    print("ğŸ“Š Classification Report:")
    print(classification_report(y_true, y_pred, target_names=label_list, zero_division=0))

import numpy as np
import torch

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# LightningModule ë‚´ë¶€ì—ì„œ ë¶„ë¥˜ê¸° êº¼ë‚´ê¸°
trained_classifier = lightning_model.model
trained_classifier.to(device)
trained_classifier.eval()

# ì„±ëŠ¥ í‰ê°€
evaluate_model(trained_classifier, test_loader, device, threshold=0.5)


#ì„±ëŠ¥ì„ ë†’ì´ë ¤ë©´ í˜„ì¬ ë‚®ê²Œ ë‚˜ì˜¨ typeì—ê²Œ ê°€ì¤‘ì¹˜ ë¶€ì—¬?