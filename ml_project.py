# -*- coding: utf-8 -*-
"""ML_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KIscABuLJunstgV1moNRMqrXxT-0y453
"""

from google.colab import drive
import glob
import json
import re
from transformers import AutoModel, AutoTokenizer, get_scheduler
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
import torch
drive.mount('/content/drive')

files = glob.glob('/content/drive/MyDrive/aihub/TL1_aihub/talksets-train-*/*.json')

from transformers import AutoTokenizer

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", trust_remote_code=True)
processed = []
label_list = ['CENSURE', 'HATE', 'DISCRIMINATION', 'SEXUAL', 'ABUSE', 'VIOLENCE', 'CRIME']
label2idx = {label: idx for idx, label in enumerate(label_list)}

for filename in files:
    with open(filename, "r", encoding="utf-8") as f:
        data = json.load(f)

    for conv in data:
        for sentence in conv.get("sentences", []):
            text = sentence.get("text")
            types = sentence.get("types", [])

            if text is None:
                print(f"⚠️ 누락된 데이터: {sentence}")
                continue

            # ✅ 토크나이즈 결과를 사전에 추가
            encoding = tokenizer(
                text,
                padding="max_length",
                truncation=True,
                max_length=128
            )

            processed.append({
                "text": text,
                "types": types,
                "input_ids": encoding["input_ids"],
                "attention_mask": encoding["attention_mask"]
            })

import torch
from torch.utils.data import Dataset

class BaseDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __getitem__(self, idx):
        item = self.data[idx]

        # label 벡터 만들기
        label_vector = [0] * len(label2idx)
        for label in item["types"]:
            if label in label2idx:
                label_vector[label2idx[label]] = 1
        # 모델에 넘기기 위해 tensor 형태로 변환, FloatTensor를 쓰는 이유는 nn.BCEWithLogitsLoss() 같은 다중 레이블 분류용 loss 함수가 float 값을 요구하기 때문.
        return {
            "input_ids": torch.tensor(item["input_ids"]),
            "attention_mask": torch.tensor(item["attention_mask"]),
            "labels": torch.FloatTensor(label_vector)
        }

    def __len__(self):
        return len(self.data)

from collections import Counter
import random

random.seed(42)
target_count = 500

label_counts = Counter()
final_data = []
used_texts = set()

for item in processed:
    item_text = item["text"]
    if item_text in used_texts:
        continue

    add_sample = False
    temp_counts = label_counts.copy()
    for label in item["types"]:
        if label in label_list and label_counts[label] < target_count:
            add_sample = True
            break

    if not add_sample:
        continue

    # 이 시점에서 샘플을 추가
    final_data.append(item)
    used_texts.add(item_text)

    for label in item["types"]:
        if label in label_list and label_counts[label] < target_count:
            label_counts[label] += 1

    if all(label_counts[l] >= target_count for l in label_list):
        break

print("✅ 라벨별 최종 수집 결과:", label_counts)
print(f"📊 최종 샘플 수: {len(final_data)}")

# test 전용 샘플 추출
test_label_counts = Counter()
test_data = []
used_texts = set()
target_test_count = 100  # label당 test용으로 100개씩

random.shuffle(final_data)  # final_data에서 추출

for item in final_data:
    item_text = item["text"]
    if item_text in used_texts:
        continue

    add = False
    for label in item["types"]:
        if label in label_list and test_label_counts[label] < target_test_count:
            add = True
            break

    if not add:
        continue

    test_data.append(item)
    used_texts.add(item_text)

    for label in item["types"]:
        if label in label_list and test_label_counts[label] < target_test_count:
            test_label_counts[label] += 1

    if all(test_label_counts[l] >= target_test_count for l in label_list):
        break

print(test_label_counts)  # ✅ 모든 라벨이 100개씩 나와야 함

from sklearn.model_selection import train_test_split

train_data, temp_data = train_test_split(final_data, test_size=0.2, random_state=42)
val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

# ✅ 그리고 바로 BaseDataset 구성
train_dataset = BaseDataset(train_data)
val_dataset = BaseDataset(val_data)
test_dataset = BaseDataset(test_data)

import torch.nn as nn

#파인 튜닝을 하는 모델, 파인튜닝은 이후 외부에서 진행됨.
class KoBERTClassifier(nn.Module):
    def __init__(self, bert_model, num_labels):
        super(KoBERTClassifier, self).__init__()
        self.bert = bert_model #KoBERT 모델을 저장
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels) # KoBERT를 CLS벡터를 받아 num_labels개의 로짓 출력

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_output = outputs.pooler_output  # CLS 토큰 벡터는 문장을 대표하는 벡터를 의미함.
        return self.classifier(cls_output)

!pip install pytorch-lightning
import pytorch_lightning as pl

def freeze_bert_layers(model, freeze_until=8):
    """
    KoBERT의 하위 encoder.layer.0 ~ encoder.layer.(freeze_until - 1)까지 freezing
    하위 레이어는 고정시키고 상위 레이어에서만 학습시킴.
    """
    for name, param in model.bert.named_parameters():
        if name.startswith("encoder.layer."):
            try:
                layer_num = int(name.split(".")[2])
                if layer_num < freeze_until:
                    param.requires_grad = False
            except ValueError:
                continue

class FocalLoss(nn.Module):
    def __init__(self, gamma=1.0):
        super().__init__()
        self.gamma = gamma

    def forward(self, inputs, targets, alpha=None):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)
        focal_loss = ((1 - pt) ** self.gamma) * BCE_loss
        if alpha is not None:
            focal_loss = alpha * focal_loss
        return focal_loss.mean()


# 학습 루프에 대한 클래스로 모델과 학습 로직을 하나의 클래스로 래핑해줌.
class HateSpeechLightningModel(pl.LightningModule):
    def __init__(self, model, lr=2e-5):
        super().__init__()
        self.model = model
        self.lr = lr

        # 클래스별 가중치 설정
        support = torch.tensor([16389, 5419, 3280, 1856, 1592, 1502, 613], dtype=torch.float32)
        alpha = (1.0 / support)
        alpha = alpha / alpha.sum() * len(support)

        # alpha를 register_buffer로 등록 → GPU로 자동 이동됨
        self.register_buffer("alpha", alpha)

        self.focal_loss = FocalLoss(gamma=1.0)

    def forward(self, input_ids, attention_mask):
        return self.model(input_ids=input_ids, attention_mask=attention_mask)

    def training_step(self, batch, batch_idx):
        logits = self(batch['input_ids'], batch['attention_mask'])
        loss = self.focal_loss(logits, batch['labels'], self.alpha)
        self.log("train_loss", loss, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        logits = self(batch['input_ids'], batch['attention_mask'])
        loss = self.focal_loss(logits, batch['labels'], self.alpha)
        self.log("val_loss", loss, prog_bar=True)
        return loss

    def configure_optimizers(self):
        return torch.optim.AdamW(self.parameters(), lr=self.lr)

import torch
import torch.nn.functional as F
from sklearn.metrics import f1_score, precision_score, recall_score
from pytorch_lightning import Trainer
from transformers import AutoModel

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)

# 1. 기존의 KoBERT base 모델 로드(파인 튜닝)
kobert_base = AutoModel.from_pretrained("monologg/kobert", trust_remote_code=True)

# 2. 분류기 모델에 base 모델 포함시킴
model = KoBERTClassifier(kobert_base, num_labels=len(label_list)).to(device)

# 3. freeze 적용은 model.bert에 해야 함.
freeze_bert_layers(model, freeze_until=8)

# 4. Lightning 래핑
lightning_model = HateSpeechLightningModel(model)

# 5. Trainer 설정 (속도 최적화 반영됨)
trainer = Trainer(
    max_epochs=5,
    accelerator="auto",
    devices=1 if torch.cuda.is_available() else None,
    precision=16,  # mixed precision → 속도 향상
    enable_checkpointing=False,  # 저장 생략 → 더 빠름
    logger=False,                # 로그 생략 → 더 빠름
    log_every_n_steps=10
)

# 6. 학습 시작
trainer.fit(lightning_model, train_loader, val_loader)

test_dataset = BaseDataset(test_data)

test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)

from sklearn.metrics import classification_report


def evaluate_model(model, dataloader, device, threshold=0.5):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].cpu().numpy()

            logits = model(input_ids, attention_mask).cpu()
            probs = torch.sigmoid(logits).numpy()

            # threshold가 scalar면 전체에 동일 적용 / array면 label마다 다르게 적용
            if isinstance(threshold, (list, np.ndarray)):
                preds = (probs > np.array(threshold)).astype(int)
            else:
                preds = (probs > threshold).astype(int)

            all_preds.append(preds)
            all_labels.append(labels)

    y_true = np.vstack(all_labels)
    y_pred = np.vstack(all_preds)

    print("📊 Classification Report:")
    print(classification_report(y_true, y_pred, target_names=label_list, zero_division=0))

import numpy as np
import torch

# 디바이스 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# LightningModule 내부에서 분류기 꺼내기
trained_classifier = lightning_model.model
trained_classifier.to(device)
trained_classifier.eval()

# 성능 평가
evaluate_model(trained_classifier, test_loader, device, threshold=0.5)


#성능을 높이려면 현재 낮게 나온 type에게 가중치 부여?